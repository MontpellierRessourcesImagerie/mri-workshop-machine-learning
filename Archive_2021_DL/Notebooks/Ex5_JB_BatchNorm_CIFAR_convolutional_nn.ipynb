{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Ex5_JB_BatchNorm_CIFAR_convolutional_nn.ipynb","provenance":[{"file_id":"1yQm4_e6cZnlNU7BMIg3lutL4HugidDyz","timestamp":1621935565000},{"file_id":"1Y5HYmAP1rTd4PthK4JnKc3FrcaKK5gew","timestamp":1619369551986},{"file_id":"1lBJ4DYIGNamm-G7SmTMq_VRRfddh-Xzd","timestamp":1619283190318}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9rG-8LoDgv8Y"},"source":["# Optimization of a CNN to classify RGB images\n","\n","The goal of this exercice is to train and optimize an artificial neural network on images from the [CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html) database. In that case, the images are classifiy according to 10 different classes : airplane, car, truck, bird, cat, dog, deer, horse, frog and ship. \n","There is several difficulties in that case :\n","- several classes are quite close (sucah as car and truck) and will require a thourough training of the network to efficiently differentiate the images\n","- the images are RGB and of very low quality"]},{"cell_type":"markdown","metadata":{"id":"0aujvzaCgv8b"},"source":["## I - Downloading the CIFAR image database\n","\n","First step - we will all the necessary libraries. "]},{"cell_type":"code","metadata":{"id":"pC5FoN7Cgv8c"},"source":["# to show images directly in the notebook\n","%matplotlib inline\n","\n","# to display tensorboard directly in the notebook\n","%load_ext tensorboard\n","\n","import numpy as np    # scientific computing \n","import matplotlib.pyplot as plt   # plotting and visualisation\n","\n","# import keras and its libraries\n","%tensorflow_version 2.x\n","from tensorflow import keras\n","from keras.models import Sequential # Model type to be used\n","from keras import regularizers\n","from keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization  # Making a model\n","from keras.utils import np_utils  # NumPy related tools\n","from keras.datasets import cifar10\n","from keras.preprocessing.image import ImageDataGenerator\n","import random "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dVZcLDPP-8sM"},"source":["Then we import the CIFAR database and display few examples. "]},{"cell_type":"code","metadata":{"id":"D27M16WO_FPZ"},"source":["(X_train, y_train), (X_val, y_val) = cifar10.load_data()\n","\n","plt.rcParams['figure.figsize'] = (5,5) # Make the figures a bit bigger\n","\n","for i in range(9):\n","    plt.subplot(3,3,i+1)\n","    num = random.randint(0, len(X_train))\n","    plt.imshow(X_train[num], cmap='gray', interpolation='none')\n","    plt.title(\"Class {}\".format(y_train[num]))\n","    \n","plt.tight_layout()\n","\n","print('')\n","print('The CIFAR database contains {} images for training and {} images for testing/validation'.format(len(X_train), len(X_val)))\n","print('X_train/X_val are composed of four dimensions. For example, X_train is {} and are respectively:'.format(X_train.shape))\n","print('- the number of images in the database')\n","print('- the 2D size of the images')\n","print('- the number of channels (for example the images are RGB and contain 3 channels)')\n","print('')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_S24FWBQDKGJ"},"source":["im = X_train[0]\n","print('The size of the image is {} pixels'.format(im.shape))\n","print('The minimum pixel value is {} and the maxium  {}'.format(np.min(im), np.max(im)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SC1mo2Bk2T5K"},"source":["## II - Formatting the data\n","\n","As already discussed for the MNIST, we need to normalize the images before working with them. In the case of the CIFAR database, the images are composed of 3 channels (RGB) and **each channel needs to be normalized separately**. "]},{"cell_type":"code","metadata":{"id":"VHnZV1ZA3W2g"},"source":["# Keep the original validation images for later\n","# ---------------------------------------------\n","\n","X_val_original = X_val\n","\n","# calculate for each channel the average and std\n","# -----------------------------------------------\n","\n","X_train = X_train.astype(float)/255\n","X_val = X_val.astype(float)/255\n","\n","Er = np.mean(X_train[:,:,:,0])\n","Eg = np.mean(X_train[:,:,:,1])\n","Eb = np.mean(X_train[:,:,:,2])\n","\n","sr = np.std(X_train[:,:,:,0])\n","sg = np.std(X_train[:,:,:,1])\n","sb = np.std(X_train[:,:,:,2])\n","\n","# Normalize data so that average is zero and std 1\n","# ------------------------------------------------\n","\n","X_train[:,:,:,0] = (X_train[:,:,:,0]-Er)/sr\n","X_train[:,:,:,1] = (X_train[:,:,:,1]-Eg)/sg\n","X_train[:,:,:,2] = (X_train[:,:,:,2]-Eb)/sb\n","\n","X_val[:,:,:,0] = (X_val[:,:,:,0]-Er)/sr\n","X_val[:,:,:,1] = (X_val[:,:,:,1]-Eg)/sg\n","X_val[:,:,:,2] = (X_val[:,:,:,2]-Eb)/sb"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VsI6iIul4dXJ"},"source":["The labels are also changed from **single digit** to **categorical or one-hot format**."]},{"cell_type":"code","metadata":{"id":"f5-1wQ484uMo"},"source":["Y_train = np_utils.to_categorical(y_train)\n","Y_val = np_utils.to_categorical(y_val)\n","\n","print(y_train[0])\n","print(Y_train[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DcQk1xYF6Qjq"},"source":["## III- Creating and training a simple network for digit classification\n","\n","We will now build a CNN network able to read the CIFAR images as input and return a vector indicating the predicted class for each image. "]},{"cell_type":"code","metadata":{"id":"2ffXLwr96zRx"},"source":["modelCNN = Sequential([\n","\n","    # Convolution Layer 1 &2\n","    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)), # 32 different 3x3 kernels -- so 32 feature maps\n","    BatchNormalization(),\n","    Conv2D(32, (3, 3), activation='relu'),\n","    BatchNormalization(),\n","    MaxPooling2D(pool_size=(2, 2)), # Pool the max values over a 2x2 kernel\n","\n","    # Convolution Layer 3 &4\n","    Conv2D(64, (3, 3), activation='relu', padding='same'), # 64 different 3x3 kernels\n","    BatchNormalization(),\n","    Conv2D(64, (3, 3), activation='relu', padding='same'),\n","    BatchNormalization(),\n","    MaxPooling2D(pool_size=(2, 2)), # Pool the max values over a 2x2 kernel\n","\n","    # Convolution Layer 5 &6\n","    Conv2D(128, (3, 3), activation='relu', padding='same'), # 128 different 3x3 kernels\n","    BatchNormalization(),\n","    Conv2D(128, (3, 3), activation='relu', padding='same'),\n","    BatchNormalization(),\n","    MaxPooling2D(pool_size=(2, 2)), # Pool the max values over a 2x2 kernel\n","\n","    Flatten(), # Flatten final 3x3x128 output matrix into a 1152-length vector \n","\n","    # Fully Connected Layer 4\n","    Dense(128, activation = 'relu'), # 128 FCN nodes\n","    Dense(10),\n","    Activation('softmax'),\n","])\n","\n","# Create the image augmentation generator\n","# ---------------------------------------\n","\n","datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n","it_train = datagen.flow(X_train, Y_train, batch_size=32)\n","steps = int(X_train.shape[0] / 32)\n","\n","# Compile the model defining the optimizer and the loss function \n","# --------------------------------------------------------------\n","\n","modelCNN.compile(optimizer = 'adam', \n","              loss='categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Return a full description of the network\n","# ----------------------------------------\n","modelCNN.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ICggImgs9nGi"},"source":["And train it !"]},{"cell_type":"code","metadata":{"id":"MmapQB9s-SEH"},"source":["#from keras.callbacks import TensorBoard  #Visulization of Accuracy and loss\n","#\n","## tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n","#%load_ext tensorboard\n","#\n","#tb_FC = TensorBoard('runs/CIFAR_v1', histogram_freq=1)\n","#%tensorboard --logdir runs\n","\n","history = modelCNN.fit_generator(it_train,\n","                       steps_per_epoch=steps,\n","                       validation_data=(X_val, Y_val),\n","                       epochs = 100,\n","                       verbose = 1)\n","\n","#history = modelCNN.fit(X_train, Y_train, \n","#          validation_data=(X_val, Y_val),\n","#          epochs=50, batch_size=32,\n","#          verbose=1)\n","          #callbacks=[tb_FC]) conda install -c conda-forge matplotlib "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ej8By2ntgv8-"},"source":["## IV- Evaluate the model accuracy :\n","\n","Calculate the accuracy of the model using the test set."]},{"cell_type":"code","metadata":{"id":"Dy4VkBkXgv8-"},"source":["test_loss, test_acc = modelCNN.evaluate(X_val, Y_val)\n","print('test_acc:', test_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NclrPuMjFSzo"},"source":["It is always useful to check the performance of the network on randomly selected images. More precisely, to improve our network, we need to understand why the network is sometimes failing. \n","Below, we are going to test the network on the validation set and select images for which the network prediction is right and other for which the predition is wrong.  "]},{"cell_type":"code","metadata":{"id":"fV3X38OjGbEN"},"source":["# The predict_classes function outputs the highest probability class\n","# according to the trained classifier for each input example.\n","# -----------------------------------------------------------\n","\n","predicted_classes = modelCNN.predict(X_val)\n","predicted_classes = np.argmax(predicted_classes, axis=1)\n","\n","# Check which items we got right / wrong\n","# --------------------------------------\n","\n","correct_indices = np.nonzero(predicted_classes == y_val[:,0])[0]\n","\n","incorrect_indices = np.nonzero(predicted_classes != y_val[:,0])[0]\n","\n","print(len(correct_indices))\n","print(len(incorrect_indices))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AYNuM9lsIhXS"},"source":["Display a few example for which the network is right"]},{"cell_type":"code","metadata":{"id":"DMx_RIXiIpRI"},"source":["plt.figure()\n","for i, correct in enumerate(correct_indices[:3]):\n","    plt.subplot(3,1,i+1)\n","    plt.imshow(X_val_original[correct], interpolation='none')\n","    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_val[correct][0]))\n","    \n","plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RNdp6TDeIzjy"},"source":["... and a few where the network is wrong!"]},{"cell_type":"code","metadata":{"id":"abXlRVKKIyuy"},"source":["plt.figure()\n","for i, incorrect in enumerate(incorrect_indices[:3]):\n","    plt.subplot(3,1,i+1)\n","    plt.imshow(X_val_original[incorrect], interpolation='none')\n","    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_val[incorrect][0]))\n","    \n","plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uw_ghkQaQpAc"},"source":["## V- Network performances\n","\n","Below the learning curves are displayed : "]},{"cell_type":"code","metadata":{"id":"xOjWIe4_TyPg"},"source":["history_dict = history.history\n","plt.rcParams['figure.figsize'] = (10,10) # Make the figures a bit bigger\n","\n","# Plot the evolution of the accuracy during the training\n","# ------------------------------------------------------\n","\n","acc_values = history_dict['accuracy']\n","val_acc_values = history_dict['val_accuracy']\n","\n","n = len(acc_values)\n","epochs = range(1, n+1)\n","\n","plt.subplot(2,1,1)\n","plt.plot(epochs, acc_values, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n","\n","# Plot the evolution of the loss during the training\n","# ------------------------------------------------------\n","\n","loss_values = history_dict['loss']\n","val_loss_values = history_dict['val_loss']\n","\n","plt.subplot(2,1,2)\n","plt.plot(epochs, loss_values, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lX1SsbSPQtCH"},"source":["Since we are working with 10 different categories, it is often useful to look at the performances of the network for each category. Below, two different tools are used to display the results :\n","- **classification report** is creating a small summary of the network performances for each category, using different metrics\n","- **confusion matrix** is returning a matrix indicating for each classes the propotion of true positive detections as well as the proportions of false negative and to which classes they are assigned. "]},{"cell_type":"code","metadata":{"id":"8PmmBpAJQtnB"},"source":["from sklearn.metrics import classification_report, confusion_matrix\n","\n","target_names = ['aiplane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","\n","print('Classification report :')\n","print('')\n","print(classification_report(np.argmax(Y_val, axis=1), predicted_classes, target_names=target_names))\n","\n","print('')\n","print('Confusion matrix :')\n","print('')\n","print(np.around(confusion_matrix(np.argmax(Y_val, axis=1), predicted_classes)/1000,1))"],"execution_count":null,"outputs":[]}]}