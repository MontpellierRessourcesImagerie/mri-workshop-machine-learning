{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk_SBGS3fbwl"
      },
      "source": [
        "# Step III : Spirale challenge\n",
        "\n",
        "In this example, the set of coordinates are now separated using two spirals. Build your own network to properly separate the two sets of data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf_IeA-ofbwn"
      },
      "source": [
        "Definition of a training data where the two classes are defined by two opposite spirals. The size of the \n",
        "training data set is 500. On top of it a Validation data set and a Testing data set are also defined, both of them\n",
        "being of size 200."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2z9i7m4fbwo"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "    \n",
        "def Training_set_spirale(a, b, Ntrain, Nval, Ntest, p_error):\n",
        "    \n",
        "    n = 0\n",
        "    N = Ntrain+Nval+Ntest\n",
        "    Data = np.zeros([N,2])\n",
        "    Labels = np.zeros([N,1])\n",
        "      \n",
        "    for n in range(0,N):\n",
        "\n",
        "        # define the data points. The label depends on the value of the random \n",
        "        # variable l and will define to which arm the point will be allocated.\n",
        "        Theta = random.uniform(0, 10*math.pi)\n",
        "        r = a*Theta+b;\n",
        "        label = random.uniform(0, 1)\n",
        "\n",
        "        Data[n,0] = (r+random.uniform(-.5, .5))*math.cos(r)\n",
        "        Data[n,1] = (r+random.uniform(-.5, .5))*math.sin(r)\n",
        "        \n",
        "        if label < 0.5 :\n",
        "            Labels[n,0] = 0\n",
        "            Data[n,0] = (r+random.uniform(-.5, .5))*math.cos(r)\n",
        "            Data[n,1] = (r+random.uniform(-.5, .5))*math.sin(r)\n",
        "        else : \n",
        "            Labels[n,0] = 1\n",
        "            Data[n,0] = -(r+random.uniform(-.5, .5))*math.cos(r)\n",
        "            Data[n,1] = -(r+random.uniform(-.5, .5))*math.sin(r)\n",
        "\n",
        "        # according to the value of the random variable e, the label will be \n",
        "        # inverted in order to introduce random errors in the data set\n",
        "        error = random.uniform(0, 1)\n",
        "        if error > 1 - p_error:\n",
        "          Labels[n,0] = 1 - Labels[n,0]\n",
        "\n",
        "    Training_data = Data[:Ntrain,]\n",
        "    Training_label = Labels[:Ntrain,]\n",
        "    Validation_data = Data[Ntrain+1:Ntrain+Nval,]\n",
        "    Validation_label = Labels[Ntrain+1:Ntrain+Nval,]\n",
        "    Testing_data = Data[Ntrain+Nval+1:N,]\n",
        "    Testing_label = Labels[Ntrain+Nval+1:N,]\n",
        "        \n",
        "    return Training_data, Testing_data, Validation_data, Training_label, Validation_label, Testing_label\n",
        "\n",
        "def Training_set_clusters(N):\n",
        "    \n",
        "    n = 0\n",
        "    Data = np.zeros([N+400,2])\n",
        "    Labels = np.zeros([N+400,1])   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRbGwxtyfbww"
      },
      "source": [
        "The training/validation/testing sets are defined below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OgSjUz3fbwy"
      },
      "source": [
        "a = 0.25;\n",
        "b = 0;\n",
        "error_probability = 0.1\n",
        "Ntrain = 200\n",
        "Nval = 100\n",
        "Ntest = 1000\n",
        "    \n",
        "Training_data, Testing_data, Validation_data, Training_label, Validation_label, Testing_label = Training_set_spirale(a, b, Ntrain, Nval, Ntest, error_probability) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training set is then plot using two different colors\n",
        "to distinguish the two classes"
      ],
      "metadata": {
        "id": "1XXELBDALHsD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# normalize the data\n",
        "# ------------------\n",
        "E = np.mean(Training_data)\n",
        "Std = np.std(Training_data)\n",
        "\n",
        "Training_data = (Training_data-E)/Std\n",
        "Testing_data = (Testing_data-E)/Std\n",
        "Validation_data = (Validation_data-E)/Std\n",
        "\n",
        "# sort the data according to their class\n",
        "# --------------------------------------\n",
        "\n",
        "Idx_class_0 = Training_label==0\n",
        "Idx_class_1 = Training_label==1\n",
        "\n",
        "X0 = Training_data[Idx_class_0[:,0],0]\n",
        "Y0 = Training_data[Idx_class_0[:,0],1]\n",
        "X1 = Training_data[Idx_class_1[:,0],0]\n",
        "Y1 = Training_data[Idx_class_1[:,0],1]\n",
        "         \n",
        "plt.rcParams['figure.figsize'] = (6,7) # Make the figures a bit bigger\n",
        "plt.plot(X0, Y0, 'r.', ms=10)\n",
        "plt.plot(X1, Y1, 'b.', ms=10)\n",
        "plt.xlabel('X train', fontsize=15)\n",
        "plt.ylabel('Y train', fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xI6O_0j7K9hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9XKoW6Ofbw3"
      },
      "source": [
        "Define the architecture of the model. You will have to decide how many layers and neurons are necessary to solve this problem. Since we are working with two classes, the activation function is \"sigmoid\" and the loss function \"binary cross-entropy\". In the end the distinction between the two classes will be made on the base of whether the output will be below or above 0,5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkD29pCcfbw4"
      },
      "source": [
        "model = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxIU7dGdfbw8"
      },
      "source": [
        "Training of the model. The number of Epoch and the minibatch size are defined below. The results at each iteraction\n",
        "are saved in order to compare the accuracy calculated for the training set and for the validation set. These data are\n",
        "saved in the variable history.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SORjKw_6fbw9"
      },
      "source": [
        "history = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujDPjHgtfbxC"
      },
      "source": [
        "The accuracy of the model is tested using the testing set of data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shJgC4jJfbxD"
      },
      "source": [
        "Results = model.predict(Testing_data)\n",
        "\n",
        "X0 = []\n",
        "X1 = []\n",
        "Y0 = []\n",
        "Y1 = []\n",
        "for n in range(len(Results)):\n",
        "\n",
        "    if Results[n]<=0.5:\n",
        "        X0.append(Testing_data[n,0])\n",
        "        Y0.append(Testing_data[n,1])\n",
        "    else:\n",
        "        X1.append(Testing_data[n,0])\n",
        "        Y1.append(Testing_data[n,1]) \n",
        "        \n",
        "plt.rcParams['figure.figsize'] = (6,7) # Make the figures a bit bigger\n",
        "plt.plot(X0, Y0, 'r.', ms=10)\n",
        "plt.plot(X1, Y1, 'b.', ms=10)\n",
        "plt.xlabel('X test', fontsize=15)\n",
        "plt.ylabel('Y test', fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At6_zKxgfbxI"
      },
      "source": [
        "In the same way, using the model.evaluate function you can test the accuracy of the model when working on the testing\n",
        "set. The second number returns the average accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgt7iAnhfbxI"
      },
      "source": [
        "Predication_accuracy = model.evaluate(Testing_data, Testing_label)\n",
        "print(Predication_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsbdiuqwfbxM"
      },
      "source": [
        "Below the accuracy for the training and validation sets are plotted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st7xcrsCfbxN"
      },
      "source": [
        "history_dict = history.history\n",
        "print( history_dict.keys() )\n",
        "\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "\n",
        "n = len(acc_values)\n",
        "epochs = range(1, n+1)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (6,6)\n",
        "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs', fontsize=15)\n",
        "plt.ylabel('Accuracy', fontsize=15)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQyqlTaOfbxQ"
      },
      "source": [
        "Below the validation loss for the training and validation sets are plotted"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6ZZALRcfbxR"
      },
      "source": [
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (6,6)\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs', fontsize=15)\n",
        "plt.ylabel('Loss', fontsize=15)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}