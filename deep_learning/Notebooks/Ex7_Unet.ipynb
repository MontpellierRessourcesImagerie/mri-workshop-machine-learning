{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Ex7_Unet",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxJNtcLHyxK7"
      },
      "source": [
        "## Training a Unet\n",
        "\n",
        "In this notebook, we will train a 2D U-net for nuclei segmentation in the Kaggle Nuclei dataset.\n",
        "\n",
        "This notebook was inspired from the EMBL notebooks from the Kreshuk lab (https://github.com/kreshuklab/teaching-dl-course-2020/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5CNxOpgx-ey"
      },
      "source": [
        "## The libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IK0BTeevRv_N"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "import sys\n",
        "import random\n",
        "import warnings\n",
        "import glob\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from itertools import chain\n",
        "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
        "from skimage.transform import resize\n",
        "from skimage.morphology import label\n",
        "\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Dropout, Lambda\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFTBWHvqLYwL"
      },
      "source": [
        "## Data loading and preprocessing\n",
        "\n",
        "For this exercise we will be using the Kaggle 2018 Data Science Bowl data again, but this time we will try to segment it with the state of the art network.\n",
        "Let's start with loading the data as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k82ftNngK8Qf"
      },
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1EbvS10-83JGNE2nlBxIV42izY1TOr115' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1EbvS10-83JGNE2nlBxIV42izY1TOr115\" -O kaggle_data.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip -qq kaggle_data.zip && rm kaggle_data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89JYxq-bL-Q3"
      },
      "source": [
        "If everything went fine, you should have folders `nuclei_train_data` and `nuclei_val_data` in your working directory. Check if it is the case by running the following cell. You should have two folders :\n",
        "- nuclei_train_data\n",
        "- nuclei_val_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO8X9gp8qKa1"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7WFCKqDUGgC"
      },
      "source": [
        "What one would normally start with in any machine learning pipeline is writing a dataset - a class that will fetch the training samples. In the previous exercises we did not have to worry about it, since we used the classic datasets available in the torchvision library. However, once you switch to using your own data, you would have to figure out how to fetch the data yourself. Luckily most of the functionality is already provided, but what you need to do is to write a class, that will actually supply the dataloader with training samples - a Dataset.\n",
        "\n",
        "For this exercise you will not have to do it yourself yet, but please carefully read through the provided class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsSW56Rh7ZwA"
      },
      "source": [
        "# Set some parameters\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "IMG_CHANNELS = 3\n",
        "TRAIN_PATH = 'nuclei_train_data/'\n",
        "TEST_PATH = 'nuclei_val_data/'\n",
        "\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='skimage')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sGsnkG0GB-n"
      },
      "source": [
        "def get_data(path, train=True):\n",
        "  # get the total number of samples\n",
        "  ids = next(os.walk(path))[1]\n",
        "  X = np.zeros((len(ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
        "  Y = np.zeros((len(ids), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
        "  print('Getting and resizing train images and masks ... ')\n",
        "  sys.stdout.flush()\n",
        "  for n, id_ in tqdm(enumerate(ids), total=len(ids)):\n",
        "    path_new = path + id_\n",
        "    # we'll be using skimage library for reading file\n",
        "    img = imread(path_new + '/images/' + id_ + '.png')[:,:,:IMG_CHANNELS]\n",
        "    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
        "    X[n] = img\n",
        "    # masks directory has multiple images - one mask per nucleus\n",
        "    mask = np.zeros((IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool)\n",
        "    for mask_file in next(os.walk(path_new + '/masks/'))[2]:\n",
        "        mask_ = imread(path_new + '/masks/' + mask_file)\n",
        "        mask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant', \n",
        "                                      preserve_range=True), axis=-1)\n",
        "        mask = np.maximum(mask, mask_)\n",
        "    Y[n] = mask\n",
        "  if train:\n",
        "    return X, Y\n",
        "  else:\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bAA6AwnUjqp"
      },
      "source": [
        "Now let's load the dataset and visualize it by calling our function:\n",
        "\n",
        "In this example, we read all images of the train folder as training data (applied SGD on) and all images of the validation folder for testing data (report performance on). Validation data (optimize hyper-parameters on) will be taken randomly from training data during the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jda9EBbmHUNy"
      },
      "source": [
        "X_train, Y_train = get_data(TRAIN_PATH, train=True)\n",
        "X_test, Y_test = get_data(TEST_PATH, train=True)\n",
        "\n",
        "X_train = X_train/255\n",
        "X_test = X_test/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyve1KK4-sS8"
      },
      "source": [
        "# Check if training data looks all right\n",
        "ix = random.randint(0, len(X_train))\n",
        "imshow(X_train[ix])\n",
        "plt.show()\n",
        "imshow(np.squeeze(Y_train[ix]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28ZjLuyGXXPL"
      },
      "source": [
        "## Building a U-NET model\n",
        "Now we need to define the architecture of the model to use. This time we will use a [U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) that has proven to steadily outperform the other architectures in segmenting biological and medical images. See also this [link](https://towardsdatascience.com/u-net-b229b32b4a71) for a discussion regarding the intuition behind the Unet.\n",
        "\n",
        "The U-net has an encoder-decoder structure: \n",
        "\n",
        "In the encoder pass, the input image is successively downsampled via max-pooling. In the decoder pass it is upsampled again via [transposed convolutions](https://towardsdatascience.com/transposed-convolution-demystified-84ca81b4baba).\n",
        "\n",
        "In adddition, it has skip connections, that bridge the output from an encoder to the corresponding decoder.\n",
        "\n",
        "Note that we are using valid convolutions here; the input to convolutions are not padded and the spatial output size after applying them decreases. Hence, the spatial output size of the network will be smaller than the spatial input size. This could be avoided by using same convolutions, which would increase the computational effort though.\n",
        "\n",
        "Compared to the paper, we will use less features (channels) to enable training the network on the CPU as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG7qWUaZDGX7"
      },
      "source": [
        "#Each block of u-net architecture consist of two Convolution layers\n",
        "# These two layers are written in a function to make our code clean\n",
        "def conv2d_block(input_tensor, n_filters, kernel_size=3):\n",
        "    # first layer\n",
        "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size),\n",
        "               padding=\"same\")(input_tensor)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    # second layer\n",
        "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), \n",
        "               padding=\"same\")(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # x = Dropout(rate=dropout_rate)(x)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmmcdboyDOfy"
      },
      "source": [
        "# The u-net architecture consists of contracting and expansive paths which\n",
        "# shrink and expands the inout image respectivly. \n",
        "# Output image have the same size of input image\n",
        "def get_unet(input_img, n_filters):\n",
        "    # contracting path\n",
        "    c1 = conv2d_block(input_img, n_filters=n_filters*4, kernel_size=3) #The first block of U-net\n",
        "    p1 = MaxPooling2D((2, 2)) (c1)\n",
        "\n",
        "    c2 = conv2d_block(p1, n_filters=n_filters*8, kernel_size=3)\n",
        "    p2 = MaxPooling2D((2, 2)) (c2)\n",
        "\n",
        "    c3 = conv2d_block(p2, n_filters=n_filters*16, kernel_size=3)\n",
        "    p3 = MaxPooling2D((2, 2)) (c3)\n",
        "\n",
        "    c4 = conv2d_block(p3, n_filters=n_filters*32, kernel_size=3)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "\n",
        "    c5 = conv2d_block(p4, n_filters=n_filters*64, kernel_size=3)\n",
        "    p5 = MaxPooling2D(pool_size=(2, 2)) (c5)\n",
        "    \n",
        "    c6 = conv2d_block(p5, n_filters=n_filters*128, kernel_size=3) # last layer on encoding path \n",
        "    \n",
        "    # expansive path\n",
        "    u7 = Conv2DTranspose(n_filters*64, (3, 3), strides=(2, 2), padding='same') (c6) #upsampling included\n",
        "    u7 = concatenate([u7, c5])\n",
        "    c7 = conv2d_block(u7, n_filters=n_filters*64, kernel_size=3)\n",
        "    # c7 = Dropout(0.4) (c7)\n",
        "\n",
        "    u8 = Conv2DTranspose(n_filters*32, (3, 3), strides=(2, 2), padding='same') (c7)\n",
        "    u8 = concatenate([u8, c4])\n",
        "    c8 = conv2d_block(u8, n_filters=n_filters*32, kernel_size=3)\n",
        "    # c8 = Dropout(0.35) (c8)\n",
        "\n",
        "    u9 = Conv2DTranspose(n_filters*16, (3, 3), strides=(2, 2), padding='same') (c8)\n",
        "    u9 = concatenate([u9, c3])\n",
        "    c9 = conv2d_block(u9, n_filters=n_filters*16, kernel_size=3)\n",
        "    # c9 = Dropout(0.3) (c9)\n",
        "\n",
        "    u10 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c9)\n",
        "    u10 = concatenate([u10, c2])\n",
        "    c10 = conv2d_block(u10, n_filters=n_filters*8, kernel_size=3)\n",
        "    # c10 = Dropout(0.25) (c10)\n",
        "\n",
        "    u11 = Conv2DTranspose(n_filters*4, (3, 3), strides=(2, 2), padding='same') (c10)\n",
        "    u11 = concatenate([u11, c1], axis=3)\n",
        "    c11 = conv2d_block(u11, n_filters=n_filters*4, kernel_size=3)\n",
        "    # c11 = Dropout(0.2) (c11)\n",
        "    \n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c11)\n",
        "    model = Model(inputs=[input_img], outputs=[outputs])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjAen-hlbLdb"
      },
      "source": [
        "## Loss and distance metrics\n",
        "\n",
        "The next step to do would be writing a loss function - a metric that will tell us how close we are to the desired output. This metric should be differentiable, since this is the value to be backpropagated. The are [multiple losses](https://lars76.github.io/neural-networks/object-detection/losses-for-segmentation/) we could use for the segmentation task.\n",
        "\n",
        "Take a moment to think which one is better to use. If you are not sure, don't forget that you can always google! Before you start implementing the loss yourself, take a look at the [losses](https://keras.io/losses/) already implemented in Keras. You can also look for implementations on GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLBwBMipbLdl"
      },
      "source": [
        "We use Binary Cross Entropy averaged over pixels as training loss.\n",
        "This loss function is similar to the cross entropy loss we have used\n",
        "for the previous classification tasks.\n",
        "\n",
        "The difference to these tasks is that we predict a single number per pixel\n",
        "(the probability of this pixel being foreground / background) instead of \n",
        "a vector per image that encodes the probabilities for several classes.\n",
        "\n",
        "We also will use the [Dice Coefficeint](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) to evaluate the network predictions.\n",
        "We can use it for validation if we interpret set $a$ as predictions and $b$ as labels. It is often used to evaluate segmentations with sparse foreground, because the denominator normalizes by the number of foreground pixels.\n",
        "The Dice Coefficient is closely related to Jaccard Index / Intersection over Union."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2ypLh2eDaFg"
      },
      "source": [
        "# the coefficient takes values in [0, 1], where 0 is the worst score, 1 is the best score\n",
        "# the dice coefficient of two sets represented as vectors a, b ca be computed as (2 *|a b| / (a^2 + b^2))\n",
        "def dice_coefficient(y_true, y_pred):\n",
        "    eps = 1e-6\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f) + eps)\n",
        "\n",
        "def loss_dice_coefficient(y_true, y_pred):\n",
        "    eps = 1e-6\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    y_true_f = float(y_true_f)\n",
        "    y_pred_f = float(y_pred_f)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return 1 - (2. * intersection) / (K.sum(y_true_f * y_true_f) + K.sum(y_pred_f * y_pred_f) + eps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C5sXunADfs-"
      },
      "source": [
        "# Creating and Compiling the model\n",
        "input_img = Input((X_train.shape[1], X_train.shape[2], 3), name='img')\n",
        "model = get_unet(input_img, n_filters=4)\n",
        "\n",
        "model.compile(optimizer=Adam(), loss=loss_dice_coefficient, metrics=[dice_coefficient])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ7o-MLtESE4"
      },
      "source": [
        "# saving the log and show it by tensorboard\n",
        "\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AESOWsYnEZCt"
      },
      "source": [
        "# Fiting the model - Note that the validation_split option is used to split the training\n",
        "# data into training and validation. Here 70% of the training data are used for \n",
        "# training, the other 30% for the validation.\n",
        "\n",
        "# datagen = ImageDataGenerator(vertical_flip=True, horizontal_flip=True)\n",
        "# it_train = datagen.flow(X_train, Y_train, batch_size=16)\n",
        "# steps = int(X_train.shape[0] / 16)\n",
        "\n",
        "# results = model.fit_generator(it_train,\n",
        "#                     steps_per_epoch=steps, epochs=100,\n",
        "#                     callbacks=[tensorboard_callback],\n",
        "#                     validation_data=(X_test, Y_test))\n",
        "\n",
        "results = model.fit(X_train, Y_train, \n",
        "                    batch_size=16, epochs=50, \n",
        "                    callbacks=[tensorboard_callback],\n",
        "                    validation_split=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKo4jqpYeHLK"
      },
      "source": [
        "## Model testing and predictions\n",
        "Now this is the time to evaluate our training model on test data which the model has never seen them before. In Keras, we can use \"model.evaluate\" to evaluate the training model where there is an avalibility of masks of test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jse1rUxRSQr3"
      },
      "source": [
        "model.evaluate(X_test,Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L17xt0U-ehA9"
      },
      "source": [
        "In Keras, \"model.predict\" is the function to predict output (masks in segmentation task or labels in classification task). Then we visualize results and visually compare the predicted masks with the ground truth."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMiuCBWAFJFI"
      },
      "source": [
        "preds_test = model.predict(X_test, verbose=1)\n",
        "# we apply a threshold on predicted mask (probability mask) to convert it to a binary mask.\n",
        "preds_test_t = (preds_test > 0.7).astype(np.uint8) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILtSa0-OQYMo"
      },
      "source": [
        "ix = random.randint(0, len(X_test))\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "plt.subplot(221)\n",
        "plt.imshow(X_test[ix,:,:,0])\n",
        "plt.title(\"input image\")\n",
        "plt.subplot(222)\n",
        "plt.imshow(np.squeeze(Y_test[ix, :, :, 0]))\n",
        "plt.title(\"ground truth\")\n",
        "plt.subplot(223)\n",
        "plt.imshow(np.squeeze(preds_test[ix, :, :, 0]))\n",
        "plt.title(\"Probability map of the predicted mask\")\n",
        "plt.subplot(224)\n",
        "plt.imshow(np.squeeze(preds_test_t[ix, :, :, 0]))\n",
        "plt.title(\"Predicted mask after thresholding\")\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}