{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rG-8LoDgv8Y"
      },
      "source": [
        "# First example with real images\n",
        "\n",
        "The goal of this exercice is to train an artificial neural network on images of **handwritten digits** and check that the trained network is able to identify and classify new images that it did not see during the training.\n",
        "We will use the **MNIST** database, which contains 60.000 images (28x28 pixels) for training and 10.000 for testing. \n",
        "\n",
        "This notebook was inspired from the EMBL notebooks from the Kreshuk lab (https://github.com/kreshuklab/teaching-dl-course-2020/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aujvzaCgv8b"
      },
      "source": [
        "## I - Downloading the MNIST image database\n",
        "\n",
        "First step - we will import the MNIST database. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC5FoN7Cgv8c"
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWNy4W8F0seU"
      },
      "source": [
        "Below, a set of 3 randomly selected images is displayed. For each image, the associated **label** (its real class, the ground truth) is indicated on the top. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3heo8_w1fjk"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import random\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (9,9)\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    num = random.randint(0, len(train_images))\n",
        "    plt.imshow(train_images[num], cmap='gray', interpolation='none')\n",
        "    plt.title(\"Class {}\".format(train_labels[num]))\n",
        "    \n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXeQSfGHnL2u"
      },
      "source": [
        "And the size of the images is indicated :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7tkvupunKtQ"
      },
      "source": [
        "im = train_images[0]\n",
        "print('The size of the image is {} pixels'.format(im.shape))\n",
        "print('The minimum pixel value is {} and the maxium  {}'.format(np.min(im), np.max(im)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SC1mo2Bk2T5K"
      },
      "source": [
        "## II - Formatting the data\n",
        "\n",
        "For the moment, we do not know how to work with 2D images. \n",
        "In the MNIST database, each image is composed of 28x28 pixels. The intensity is encoded in 8-bit, meaning that the intensity value of each pixel will be between 0 and 255=2â¸-1. \n",
        "For the input layer of the network, we need to transform those **images** into a **single-column tensor composed of 28x28=784 elements**. Each image is going to be **reshaped** or **flatten**. \n",
        "\n",
        "The data are also **normalized** so that all the values will be between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHnZV1ZA3W2g"
      },
      "source": [
        "# the images are flatten into one single tensor containing 784 elements. The \n",
        "# images are also converted to float to allow for the normalization (else,\n",
        "# if we keep the integer formation, the result of the normalization will be\n",
        "# either 0 or 1)\n",
        "# -------------\n",
        "\n",
        "n_pixel = np.prod(im.shape)\n",
        "X_train = train_images.reshape(train_images.shape[0], n_pixel).astype('float32')\n",
        "X_val = test_images.reshape(test_images.shape[0], n_pixel).astype('float32')\n",
        "\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "# ----------------------------------\n",
        "\n",
        "X_train /= 255\n",
        "X_val /= 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsI6iIul4dXJ"
      },
      "source": [
        "The labels are also changed from **single digit** to **categorical or one-hot format**. Example:\n",
        "\n",
        "class \"0\" : [**1** 0 0 0 0 0 0 0 0 0]\n",
        "\n",
        "class \"1\" : [0 **1** 0 0 0 0 0 0 0 0]\n",
        "\n",
        "etc.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5-1wQ484uMo"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "\n",
        "Y_train = np_utils.to_categorical(train_labels)\n",
        "Y_val = np_utils.to_categorical(test_labels)\n",
        "\n",
        "for n in range(5):\n",
        "  print(\"The previous label was \" + str(train_labels[n]) + \" and is now \" + str(Y_train[n]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcQk1xYF6Qjq"
      },
      "source": [
        "## III- Creating and training a simple network for digit classification\n",
        "\n",
        "We will now build a simple network able to read the modify images as input and return a vector (in the \"one-hot\" format) indicating the predicted class for the image. \n",
        "For now, the structure of this network remains very similar to what we have already seen yesterday. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ffXLwr96zRx"
      },
      "source": [
        "# Import the keras libraries \n",
        "# --------------------------\n",
        "from ... import ...\n",
        "\n",
        "# Define the architecture of the network. Note that the activation function for \n",
        "# the last layer is \"softmax\", which gives the probability for each of the 10 classes\n",
        "# ---------------------------\n",
        "\n",
        "model = ...\n",
        "\n",
        "# Compile the model defining the optimizer and the loss function \n",
        "# --------------------------------------------------------------\n",
        "model.compile(...)\n",
        "\n",
        "# Return a full description of the network\n",
        "# ----------------------------------------\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICggImgs9nGi"
      },
      "source": [
        "(Optional) \n",
        "\n",
        "In order to find the best network architecture for your problem, many training will be performed and compared. TensorFlow offers several vizualization tools to facilitate this task. \n",
        "\n",
        "TensorBoard provides the visualization and tooling needed for machine learning experimentation:\n",
        "*   Tracking and visualizing metrics such as loss and accuracy\n",
        "*   Visualizing the model graph (ops and layers)\n",
        "*   Viewing histograms of weights, biases, or other tensors as they change over time\n",
        "*   Projecting embeddings to a lower dimensional space\n",
        "*   Displaying images, text, and audio data\n",
        "*   Profiling TensorFlow programs\n",
        "*   And much more\n",
        "\n",
        "After running the following block, you should click on the generated link."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmapQB9s-SEH"
      },
      "source": [
        "from keras.callbacks import TensorBoard  #Visulization of Accuracy and loss\n",
        "\n",
        "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "%load_ext tensorboard\n",
        "\n",
        "tb_FC = TensorBoard('runs/MNIST_dense_model', histogram_freq=1)\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERuIlVX3-wSv"
      },
      "source": [
        "Finallly, we are going to train the model (with `model.fit()` ) and follow in real-time how the network is able to learn. We send for training a batch of 32 images each iteration, until all traing images are used. We then repeat this 50 times (i.e. 50 epochs). Look each epoch how the loss decreases and the accuracy increases, both on the training and validation (`val_*`) images. The best we aim for is `val_accuracy = 1` (but >0.9 is already not bad)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siCDK17A-8wC"
      },
      "source": [
        "history = model.fit(X_train, Y_train, \n",
        "          validation_data=(X_val, Y_val),\n",
        "          epochs=50, batch_size=32,\n",
        "          verbose=1,\n",
        "          callbacks=[tb_FC])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej8By2ntgv8-"
      },
      "source": [
        "## IV- Evaluate the model accuracy :\n",
        "\n",
        "After training is done, you can calculate the accuracy of the model using the test set. You feed the trained model with all the test images (which have their respective labels known), and see how good are the predictions of the model, i.e. how often it predicts the right class. It will be likely not too different from the `val_accuracy` obtained at the end of the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy4VkBkXgv8-"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(X_val, Y_val)\n",
        "print('test_acc:', test_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NclrPuMjFSzo"
      },
      "source": [
        "It is always useful to check the performance of the network on randomly selected images. More precisely, to improve our network, we need to understand why the network is sometimes failing. \n",
        "Below, we are going to test the network on the validation set (i.e. images not seen during training) and select images for which the network prediction is right and other for which the predition is wrong.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV3X38OjGbEN"
      },
      "source": [
        "# The predict_classes function outputs the highest probability class\n",
        "# according to the trained classifier for each input example.\n",
        "# -----------------------------------------------------------\n",
        "\n",
        "predicted_classes = model.predict(X_val)\n",
        "predicted_classes = np.argmax(predicted_classes, axis=1)\n",
        "\n",
        "# Check which items we got right / wrong\n",
        "# --------------------------------------\n",
        "\n",
        "correct_indices = np.nonzero(predicted_classes == test_labels)[0]\n",
        "\n",
        "incorrect_indices = np.nonzero(predicted_classes != test_labels)[0]\n",
        "\n",
        "print(len(correct_indices))\n",
        "print(len(incorrect_indices))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYNuM9lsIhXS"
      },
      "source": [
        "Display a few example for which the network is right"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMx_RIXiIpRI"
      },
      "source": [
        "plt.figure()\n",
        "for i, correct in enumerate(correct_indices[:6]):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(X_val[correct].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], test_labels[correct]))\n",
        "    \n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNdp6TDeIzjy"
      },
      "source": [
        "... and a few where the network is wrong!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abXlRVKKIyuy"
      },
      "source": [
        "plt.figure()\n",
        "for i, incorrect in enumerate(incorrect_indices[:6]):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.imshow(X_val[incorrect].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], test_labels[incorrect]))\n",
        "    \n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIqa1enqKfao"
      },
      "source": [
        "And finally the accuracy and loss along the training epochs can be plotted :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D2_jn4kKpUH"
      },
      "source": [
        "history_dict = history.history\n",
        "\n",
        "# Plot the evolution of the accuracy during the training\n",
        "# ------------------------------------------------------\n",
        "\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "\n",
        "n = len(acc_values)\n",
        "epochs = range(1, n+1)\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs', fontsize=15)\n",
        "plt.ylabel('Accuracy', fontsize=15)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the evolution of the loss during the training\n",
        "# ------------------------------------------------------\n",
        "\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs', fontsize=15)\n",
        "plt.ylabel('Loss', fontsize=15)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M8cRJVy0qnG"
      },
      "source": [
        "## V- Introduction to CNN = Convolutional Neural Network\n",
        "\n",
        "Let's move to the next step, improving the model performace: convolutional nets.\n",
        "\n",
        "In order to work with densely connected network, the images need to be reshape and flatten. **Useful spatial information is lost in this process**. \n",
        "**Convolutional network** has been introduced to work with 2D/3D images and analyze the spatial context around the pixels. Such network are able to **learn which features of your image (curve, intensity, shape, etc.) are important for the classficiation**. \n",
        "The network will learn **\"filters\"** that will be applied to each image in order to highlight specific features of your images.   \n",
        "\n",
        "\n",
        "\n",
        "To start we need to reload the MNIST database and import new libraries specific to CNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdtXw9Aq2CEE"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import random\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten\n",
        "from keras.models import Sequential # model type to be used\n",
        "from keras import Model # used for the visualization of the features maps\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import mnist\n",
        "from keras.callbacks import TensorBoard  #Visulization of Accuracy and loss\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_zsqWXP2zg3"
      },
      "source": [
        "A normalization step is again required, though this time we are keeping the shape of the image (28x28 pixels). \n",
        "The labels are also changed again to the one-hot format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eg_BkLTD3ACm"
      },
      "source": [
        "X_train = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "X_val = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')\n",
        "\n",
        "# normalize inputs from 0-255 to 0-1\n",
        "# ----------------------------------\n",
        "\n",
        "X_train /= 255\n",
        "X_val /= 255\n",
        "\n",
        "# change the label from single digit to one-hot format\n",
        "# ----------------------------------------------------\n",
        "\n",
        "Y_train = np_utils.to_categorical(train_labels)\n",
        "Y_val = np_utils.to_categorical(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY1YHxfp4VW_"
      },
      "source": [
        "Now, we are creating a simple CNN to analyze the images!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjwHuIAz4vno"
      },
      "source": [
        "modelCNN = Sequential([\n",
        "    \n",
        "    # Convolution Layer 1\n",
        "    Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)), # 16 different 3x3 kernels -- so 16 feature maps\n",
        "    MaxPooling2D(pool_size=(2, 2)), # Pool the max values over a 2x2 kernel\n",
        "\n",
        "    # Convolution Layer 2\n",
        "    Conv2D(16, (3, 3), activation='relu'), # 16 different 3x3 kernels \n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Convolution Layer 3\n",
        "    Conv2D(16, (3, 3), activation='relu'), # 16 different 3x3 kernels\n",
        "\n",
        "    Flatten(), # Flatten final 3x3x16 output matrix into a 144-length vector \n",
        "\n",
        "    # Fully Connected Layer 4\n",
        "    Dense(15), # 15 FCN nodes\n",
        "    Activation('relu'),\n",
        "    Dense(10), # Necessary for the last layer since we have 10 classes\n",
        "    Activation('softmax'),\n",
        "])\n",
        "modelCNN.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LxZhzxU7K2c"
      },
      "source": [
        "And finally we are training this network :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oOnMPi07PIx"
      },
      "source": [
        "%load_ext tensorboard\n",
        "\n",
        "tb_CNN = TensorBoard('runs/CNN_model', histogram_freq=1)\n",
        "%tensorboard --logdir runs\n",
        "\n",
        "modelCNN.compile(loss='categorical_crossentropy', \n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = modelCNN.fit(X_train, Y_train, \n",
        "          validation_data=(X_val, Y_val),\n",
        "          epochs=10, batch_size=32,\n",
        "          verbose=1,\n",
        "          callbacks=[tb_CNN])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbH9WFQLVUnY"
      },
      "source": [
        "history_dict = history.history\n",
        "\n",
        "# Plot the evolution of the accuracy during the training\n",
        "# ------------------------------------------------------\n",
        "\n",
        "acc_values = history_dict['accuracy']\n",
        "val_acc_values = history_dict['val_accuracy']\n",
        "\n",
        "n = len(acc_values)\n",
        "epochs = range(1, n+1)\n",
        "\n",
        "plt.rcParams['figure.figsize'] = (9,9) # Make the figures a bit bigger\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(epochs, acc_values, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs', fontsize=15)\n",
        "plt.ylabel('Accuracy', fontsize=15)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the evolution of the loss during the training\n",
        "# ------------------------------------------------------\n",
        "\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs', fontsize=15)\n",
        "plt.ylabel('Loss', fontsize=15)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOEDszxDSyCb"
      },
      "source": [
        "##VI- Kernel and features map \n",
        "\n",
        "Finally we can visualize what the network is learning. Below, a simple script is used to display the filters that are learnt by the network as well as the feature maps. \n",
        "\n",
        "For more details regarding the code you can go [here](https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/) and for a user-friendly visualization app [here](https://www.cs.ryerson.ca/~aharley/vis/conv/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be1hXQ08S5PX"
      },
      "source": [
        "# Indicate the layer you wish to observe (works only for the conv layers)\n",
        "n_layer = 0\n",
        "\n",
        "# Retrieve the weights from the chosen layer\n",
        "filters, biases = modelCNN.layers[n_layer].get_weights()\n",
        "print('The shape of filters is {}'.format(filters.shape))\n",
        "\t\n",
        "# normalize filter values between 0-1\n",
        "f_min, f_max = filters.min(), filters.max()\n",
        "filters = (filters - f_min) / (f_max - f_min)\n",
        "\n",
        "# plot the 16 filters from the convolutional layer\n",
        "\n",
        "for i in range(16):\n",
        "\t# get the filter\n",
        "\tf = filters[:, :, :, i]\n",
        "\n",
        "\t# plot each filter in gray scale\n",
        "\n",
        "\tax = plt.subplot(4, 4, i+1)\n",
        "\tax.set_xticks([])\n",
        "\tax.set_yticks([])\n",
        "\tplt.imshow(f[:,:,0], cmap='gray')\n",
        " \n",
        "# show the figure\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FraKm5waYgA6"
      },
      "source": [
        "# Select one image from the validation set\n",
        "n_im = 0\n",
        "\n",
        "im = X_val[n_im]\n",
        "im = np.expand_dims(im, axis=0) \n",
        "\n",
        "# get feature maps by redefining the model to output only the first hidden layer\n",
        "n_layer = 1\n",
        "model = Model(inputs=modelCNN.inputs, outputs=modelCNN.layers[n_layer].output)\n",
        "\n",
        "feature_maps = model.predict(im)\n",
        "print(\"The size of features map is {}. There are as many maps as the number of filters in the convolution layer.\".format(feature_maps.shape))\n",
        "\t\n",
        "# plot all 16 maps\n",
        "\n",
        "for i in range(16):\n",
        "\tax = plt.subplot(4, 4, i+1)\n",
        "\tax.set_xticks([])\n",
        "\tax.set_yticks([])\n",
        "\t# plot filter channel in grayscale\n",
        "\tplt.imshow(feature_maps[0, :, :, i], cmap='gray')\n",
        "\n",
        "# show the figure\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}