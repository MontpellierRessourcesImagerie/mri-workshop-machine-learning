{"cells":[{"cell_type":"markdown","metadata":{"id":"NQRNRzBUrmIx"},"source":["# Red blood cell classifier : practical application of a convNet\n","\n","With this example, you will build your own convNet in order to perform a specific task : classifying images of red blood cells (RBC) of either good or bad quality.\n","\n","Data were kindly provided by Viviana Claveria & Manouk Abkarian from the Centre of Biologie Structurale, Montpellier (FRANCE). \n","\n","This notebook was inspired from the EMBL notebooks from the Kreshuk lab (https://github.com/kreshuklab/teaching-dl-course-2020/)."]},{"cell_type":"markdown","metadata":{"id":"3XbYo16drmJE"},"source":["## I - Data importation\n","\n","The data are RGB images saved in the png format on a google drive. All images are already sorted according to two classes :\n","- good RBC (1)\n","- bad RBC (0)\n","\n","\n","The first step is to load all the python packages we will use in the notebook:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9EQkNfa8rmJG"},"outputs":[],"source":["import os\n","from glob import glob\n","import random\n","import sys\n","import warnings\n","import numpy as np\n","from tqdm import tqdm\n","\n","from skimage.io import imread, imshow, imread_collection, concatenate_images\n","from skimage.transform import resize\n","from skimage.morphology import label\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Input\n","from keras.layers import RandomFlip, Resizing, Rescaling, RandomRotation\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"4pE-vsf3ujF-"},"source":["If your data are save on your google drive, you first need to connect google drive to google collab and move to the folder containing the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gt7ObWPAsDsd"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","base_dir = '/content/drive/MyDrive/Deep_learning_formation_MRI/Doc_JB_2022/Notebooks for workshop /Data/RBC_classification/RBC_dataset_simplified'\n","os.chdir(base_dir)\n","%ls"]},{"cell_type":"markdown","metadata":{"id":"ZdZ9rx8Jo8oc"},"source":["Since all the images are different and do not have the same X,Y dimensions we will define a set of parameters to homogeneize the training/testing sets. For example, below the width and height of all images will be set to IMG_WIDTH and IMG_HEIGHT respectively. In our case, we are working with RGB images, the number of channels is therefore set to 3.\n","\n","Note that **the path to the different folders will need to be updated** accordingly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jo2UsUAB5SvP"},"outputs":[],"source":["# Set the image size\n","# -----------------\n","IMG_WIDTH = 85\n","IMG_HEIGHT = 85\n","IMG_CHANNEL = 3\n","\n","# Define the path where the data are saved\n","# ----------------------------------------\n","\n","goodRBC_train_PATH = 'train/good_RBC/'\n","badRBC_train_PATH = 'train/bad_RBC/'\n","\n","goodRBC_val_PATH = 'validation/good_RBC/'\n","badRBC_val_PATH = 'validation/bad_RBC/'\n","\n","goodRBC_test_PATH = 'test/good_RBC/'\n","badRBC_test_PATH = 'test/bad_RBC/'"]},{"cell_type":"markdown","metadata":{"id":"lnpIfpz2pXMM"},"source":["The following method **get_data** is used to download the images and convert them to the right format (according to the parameters defined above)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jqVGyk4G_1px"},"outputs":[],"source":["def get_data(path):\n","\n","  # get the total number of samples\n","  # -------------------------------\n","\n","  ids = next(os.walk(path))[2]\n","  X = np.zeros((len(ids), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNEL), dtype=np.uint8)\n","  img_size = np.zeros((len(ids),2))\n","\n","  # sys.stdout.flush()\n","  \n","  # select only the first n_im images\n","  # ---------------------------------\n","\n","  for n, id_ in tqdm(enumerate(ids), total=len(ids)):\n","    path_new = os.path.join(path, id_)\n","\n","    # we'll be using skimage library for reading file and make sure all the images\n","    # have the same dimensions\n","    # -------------------------\n","\n","    img = imread(path_new)\n","    img_size[n,:] = img.shape[0:1]\n","    img = resize(img, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n","\n","    if len(img.shape) == 3:\n","      X[n] = img\n","    else:\n","      img = np.stack((img,)*3, axis=-1)\n","      X[n] = img\n","\n","  # Return the median width/height of the set of images \n","  print(f'median images width : {np.median(img_size[:,0])} +/- {np.std(img_size[:,0])}')\n","  print(f'median images heigth : {np.median(img_size[:,1])} +/- {np.std(img_size[:,1])}')\n","\n","  return X"]},{"cell_type":"markdown","metadata":{"id":"6rehyVFIpx6b"},"source":["The training and testing set are defined below. Note that we are also building the ground truth accordingly.\n","**Note that it takes ~20-30min to load the data.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALHaQ8Aj_6S2"},"outputs":[],"source":["# Load the training set - the images are classified into two different folders. \n","# Below, the \"good\" images are associated to a label equal to 1. The label for \n","# the \"bad\" images is set to 0.  \n","# ----------------------------\n","Good_RBC = get_data(goodRBC_train_PATH)\n","y_good = np.ones((Good_RBC.shape[0],))\n","Bad_RBC = get_data(badRBC_train_PATH)\n","y_bad = np.zeros((Bad_RBC.shape[0],))\n","\n","X_train = np.concatenate((Good_RBC,Bad_RBC), axis=0)\n","Y_train = np.concatenate((y_good,y_bad), axis=0)\n","\n","# Load the validation set\n","# -----------------------\n","Good_RBC_validation = get_data(goodRBC_val_PATH)\n","y_good = np.ones((Good_RBC_validation.shape[0],))\n","Bad_RBC_validation = get_data(badRBC_val_PATH)\n","y_bad = np.zeros((Bad_RBC_validation.shape[0],))\n","\n","X_validation = np.concatenate((Good_RBC_validation,Bad_RBC_validation), axis=0)\n","Y_validation = np.concatenate((y_good,y_bad), axis=0)\n","\n","# Load the test set\n","# -----------------\n","Good_RBC_test = get_data(goodRBC_test_PATH)\n","y_good = np.ones((Good_RBC_test.shape[0],))\n","Bad_RBC_test = get_data(badRBC_test_PATH)\n","y_bad = np.zeros((Bad_RBC_test.shape[0],))\n","\n","X_test = np.concatenate((Good_RBC_test,Bad_RBC_test), axis=0)\n","Y_test = np.concatenate((y_good,y_bad), axis=0)\n"]},{"cell_type":"markdown","metadata":{"id":"1NUf3R0KeH4L"},"source":["Return the composition of the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tncgWMRneG3h"},"outputs":[],"source":["print(f'The train dataset is composed of {Good_RBC.shape[0]} images belonging to the \"goodRBC\" class and {Bad_RBC.shape[0]} images belonging to the \"badRBC\" class')\n","print(f'The validation dataset is composed of {Good_RBC_validation.shape[0]} images belonging to the \"goodRBC\" class and {Bad_RBC_validation.shape[0]} images belonging to the \"badRBC\" class')\n","print(f'The test dataset is composed of {Good_RBC_test.shape[0]} images belonging to the \"goodRBC\" class and {Bad_RBC_test.shape[0]} images belonging to the \"badRBC\" class')"]},{"cell_type":"markdown","metadata":{"id":"pnWY37lJ1bmd"},"source":["## II- Data vizualization :\n","\n","Display a few examples of images belonging to the \"GoodRBC\" class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XYidlV5Gvk7j"},"outputs":[],"source":["plt.rcParams['figure.figsize'] = (9,9) # Make the figures a bit bigger\n","\n","for i in range(9):\n","    plt.subplot(3,3,i+1)\n","    num = random.randint(0, len(Good_RBC))\n","    im = Good_RBC[num]\n","    plt.imshow(im)\n","    \n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"QnV8EAur1klZ"},"source":["And the same for the \"BadRBC\" class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bKs348GS1ot4"},"outputs":[],"source":["plt.rcParams['figure.figsize'] = (9,9) # Make the figures a bit bigger\n","\n","for i in range(9):\n","    plt.subplot(3,3,i+1)\n","    num = random.randint(0, len(Bad_RBC))\n","    im = Bad_RBC[num]\n","    plt.imshow(im)\n","    \n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ST02zYbxrmJP"},"source":["## III- Definition of the model and training\n","\n","Define the model and the compilation options. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DzV8Z8nyrmJQ"},"outputs":[],"source":["model = ...\n","\n","model.compile(...)\n","\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"gvXQjHmsrmJR"},"source":["Define the model and the compilation options. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"duPj2CGOrmJS"},"outputs":[],"source":["history = ..."]},{"cell_type":"markdown","metadata":{"id":"Fl9W_ynNrmJW"},"source":["Display the loss function during the training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nYto0pbGrmJW"},"outputs":[],"source":["history_dict = history.history\n","\n","loss_values = history_dict['loss']\n","val_loss_values = history_dict['val_loss']\n","\n","n = len(loss_values)\n","epochs = range(1, n+1)\n","\n","plt.rcParams['figure.figsize'] = (7,7) # Make the figures a bit bigger\n","plt.plot(epochs, loss_values, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs', fontsize=15)\n","plt.ylabel('Loss', fontsize=15)\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"00hpazBzrmJY"},"source":["The accuracy of the model is tested using the testing set of data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_DqMUUdHrmJY"},"outputs":[],"source":["acc_values = history_dict['accuracy']\n","val_acc_values = history_dict['val_accuracy']\n","\n","n = len(acc_values)\n","epochs = range(1, n+1)\n","\n","plt.rcParams['figure.figsize'] = (7,7) # Make the figures a bit bigger\n","plt.plot(epochs, acc_values, 'bo', label='Training acccuracy')\n","plt.plot(epochs, val_acc_values, 'b', label='Validation accuracy')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs', fontsize=15)\n","plt.ylabel('accuracy', fontsize=15)\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"FI_aZSkn1CA4"},"source":["Evaluate the model :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cverlFBr1CA4"},"outputs":[],"source":["test_loss, test_acc = model.evaluate(X_test, Y_test)\n","print('test_acc:', test_acc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sfaDrk2r1CA5"},"outputs":[],"source":["# The predict_classes function outputs the highest probability class\n","# according to the trained classifier for each input example.\n","# -----------------------------------------------------------\n","\n","predicted_classes = model.predict(X_test)\n","predicted_classes = np.round(predicted_classes) # defines the threshold between the two classes at 0.5\n","predicted_classes = predicted_classes.flatten()\n","\n","# Check which items we got right / wrong\n","# --------------------------------------\n","\n","correct_indices = np.nonzero(predicted_classes[:] == Y_test[:])[0]\n","incorrect_indices = np.nonzero(predicted_classes[:] != Y_test[:])[0]\n","\n","print(len(correct_indices))\n","print(len(incorrect_indices))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WAsqhveQ1CA5"},"outputs":[],"source":["plt.rcParams['figure.figsize'] = (9,9) # Make the figures a bit bigger\n","\n","for i in range(9):\n","    plt.subplot(3,3,i+1)\n","    num = random.randint(0, len(correct_indices))\n","    plt.imshow(X_test[correct_indices[num]], interpolation='none')\n","    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct_indices[num]],\n","                                              Y_test[correct_indices[num]]))\n","    \n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fv8MmJV21CA6"},"outputs":[],"source":["plt.rcParams['figure.figsize'] = (9,9) # Make the figures a bit bigger\n","\n","for i in range(9):\n","    plt.subplot(3,3,i+1)\n","    num = random.randint(0, len(incorrect_indices))\n","    plt.imshow(X_test[incorrect_indices[num]], interpolation='none')\n","    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect_indices[num]], \n","                                              Y_test[incorrect_indices[num]]))\n","    \n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"BNDvol02smgM"},"source":["## IV- Transfer learning\n","\n","In the following section, we will see how transfer learning can be used to improve the performances of a classifier. The idea is to  used a previously trained network (such as VGG16) that was already trained on thousands of images and able to recognize hundred of thousands of different features on images.\n","\n","By adding new layers at the end of the pre-trained network, we can use the features recognition property of this network and applied it to a completely new problem. "]},{"cell_type":"markdown","metadata":{"id":"l6a8KNvUuSTd"},"source":["The first step is to load the pre-trained VGG16 network. There are many different available model in keras (https://keras.io/api/applications/) and they all can be used for transfer learning. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p08M5dZwsl1X"},"outputs":[],"source":["# from keras.applications import the vgg16 network pretrained on the imagenet \n","# database. The include_top option is set to zero, meaning that the classifier \n","# part (composed of a dense network) is removed.\n","# ---------------------------------------------- \n","conv_base = keras.applications.vgg16.VGG16(\n","    include_top=False,\n","    weights='imagenet',\n","    input_shape=(IMG_WIDTH, IMG_HEIGHT, IMG_CHANNEL)\n",")\n","\n","conv_base.summary()"]},{"cell_type":"markdown","metadata":{"id":"ODAvzNKiup2p"},"source":["And then we will build our model around the VGG16 :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTuh0eMpuvMb"},"outputs":[],"source":["model_VGG = Sequential([\n","    \n","    # Initialization, normalization and image augmentation \n","    Input(shape=(IMG_WIDTH, IMG_HEIGHT, IMG_CHANNEL)),\n","    Rescaling(scale=1./255),\n","    RandomFlip(mode=\"horizontal_and_vertical\"),\n","    \n","    # Add the VGG16 network without the classifier\n","    conv_base,\n","\n","    Flatten(),\n","    \n","    # Create the fully connected layers for the final classification\n","    Dense(256, activation = 'relu'), # 256 FCN nodes\n","    Dropout(0.5),\n","    Dense(128, activation = 'relu'), # 128 FCN nodes\n","    Dropout(0.5),\n","    Dense(1, activation = 'sigmoid'),\n","])\n","model_VGG.summary()"]},{"cell_type":"markdown","metadata":{"id":"aaz6DPmWuzJf"},"source":["By default, all the parameters of the VGG16 network can be retrained. For transfer learning, we need to define which part of the network will be trained, and which part of the network will be \"frozen\" (i.e. will keep the same values for the weights after being trained on the imageNet database). \n","In our case, only the last convolution block of the VGG and the densely connected part will be trained :"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ugjEmuTu3fM"},"outputs":[],"source":["# The conv_base is composed of 19 layers. The last 4 layers are related to \n","# block5. Therefore, only the four last payers will be set to \"trainable\".\n","# ------------------------------------------------------------------------\n","\n","conv_base.trainable = True\n","for n in range(15):\n","  conv_base.layers[n].trainable = False\n","\n","# Now, when looking at the network summary, the global architecture did not \n","# change. However, the number of trainable parameters dropped by almost half.\n","model_VGG.summary()\n","\n","# Compile the new network\n","model_VGG.compile(optimizer = 'adam', \n","            loss='binary_crossentropy',\n","            metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"ObsKCEasvqMJ"},"source":["And finally train the new model and save it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RJTOzDafvmPb"},"outputs":[],"source":["history = model_VGG.fit(X_train, Y_train,\n","                    epochs = 50,\n","                    batch_size = 64,\n","                    validation_data=(X_validation, Y_validation),\n","                    shuffle = True)\n","\n","# Save the model\n","# --------------\n","\n","# model.save('RBC_classification_VGG16_1.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NIt6pVLM1CBA"},"outputs":[],"source":["history_dict = history.history\n","\n","loss_values = history_dict['loss']\n","val_loss_values = history_dict['val_loss']\n","\n","n = len(loss_values)\n","epochs = range(1, n+1)\n","\n","plt.rcParams['figure.figsize'] = (7,7) # Make the figures a bit bigger\n","plt.plot(epochs, loss_values, 'bo', label='Training loss')\n","plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs', fontsize=15)\n","plt.ylabel('Loss', fontsize=15)\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I5-nWUOS1CBA"},"outputs":[],"source":["acc_values = history_dict['accuracy']\n","val_acc_values = history_dict['val_accuracy']\n","\n","n = len(acc_values)\n","epochs = range(1, n+1)\n","\n","plt.rcParams['figure.figsize'] = (7,7) # Make the figures a bit bigger\n","plt.plot(epochs, acc_values, 'bo', label='Training acccuracy')\n","plt.plot(epochs, val_acc_values, 'b', label='Validation accuracy')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs', fontsize=15)\n","plt.ylabel('accuracy', fontsize=15)\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uPzXTWkBL2Tv"},"outputs":[],"source":["test_loss, test_acc = model_VGG.evaluate(X_test, Y_test)\n","print('test_acc:', test_acc)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1kqRdHGbbWkwEquxL1qWRhKvZi8XyT4Gq","timestamp":1668694105101}]},"kernelspec":{"display_name":"DL","language":"python","name":"dl"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}