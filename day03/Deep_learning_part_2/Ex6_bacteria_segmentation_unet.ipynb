{"cells":[{"cell_type":"markdown","source":["# Bacteria cell segmentation : practical application of a Unet\n","\n","With this example, you will a very simple application of a Unet for semantic segmentation. The data are 2d images of bacteria, acquired using brightfield imaging modality. \n","\n","Data were kindly provided by Caroline Clerté from the Centre of Biologie Structurale, Montpellier (FRANCE). \n","\n","This notebook was inspired from the EMBL notebooks from the Kreshuk lab (https://github.com/kreshuklab/teaching-dl-course-2020/)."],"metadata":{"id":"dswXOrJuMhRC"}},{"cell_type":"markdown","source":["## I - Data importation\n","\n","The data are 2d 16bit images saved in the tif format on a google drive. All images are paired :\n","- one raw image \n","- one labelled image displaying three pixel values : 0 = background ; 1 = cell ; 2 = cell contour\n","\n","The first step is to load all the python packages we will use in the notebook:"],"metadata":{"id":"Se0i5oZHNDMj"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"W8HsNqNF7JQu"},"outputs":[],"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","\n","import os\n","import sys\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from glob import glob\n","from tqdm import tqdm\n","from skimage.io import imread, imshow, imread_collection, concatenate_images, imsave\n","\n","from skimage.measure import label, regionprops\n","\n","from keras.models import Model, load_model\n","from keras.layers.core import Dropout, Lambda\n","from keras.layers.convolutional import Conv2D, Conv2DTranspose\n","from keras.layers.pooling import MaxPooling2D\n","from keras.layers import Concatenate, RandomFlip, RandomContrast\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","from keras import backend as K\n","from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout\n","from keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.utils import normalize\n","\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqzAJABgDIu-"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"RVgoDlVw96Hx"},"source":["All the images are 128 x 128 pixels and are already sorted and are already sorted in two separate folders:\n","- one for the training data (111 images)\n","- one for the test data (19 images)\n","\n","No validation set was available for this set of data. \n","\n","Note that **the path to the different folders will need to be updated** accordingly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9hvDIqe0-FGX"},"outputs":[],"source":["IMG_WIDTH = 128\n","IMG_HEIGHT = 128\n","IMG_CHANNELS = 1\n","TRAIN_PATH = '/content/drive/MyDrive/Deep_learning_formation_MRI/Doc_JB_2022/Notebooks for workshop /Data/Bacteria_segmentation/128x128_dataset/training_data'\n","TEST_PATH = '/content/drive/MyDrive/Deep_learning_formation_MRI/Doc_JB_2022/Notebooks for workshop /Data/Bacteria_segmentation/128x128_dataset/testing_data'"]},{"cell_type":"markdown","source":["The following method **get_data** is used to download the images and convert them to the right format (according to the parameters defined above)."],"metadata":{"id":"4jKbeHqqPBwe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5rB0UfoMFmbJ"},"outputs":[],"source":["def get_data(data_path, mask_path=None):\n","\n","  # look for all the tif files in the selected folder\n","  imgs = sorted(glob(data_path + os.sep + '*.tif'))\n","  X = np.zeros((len(imgs), IMG_HEIGHT, IMG_WIDTH))\n","  \n","  # load the raw data and save them in array X\n","  print('Loading the raw images ... ')\n","  for n, element in tqdm(enumerate(imgs), total = len(imgs)):\n","    img = imread(element)\n","    X[n,:,:] = img\n","    \n","  # perform the same for the labelled images and save the labelled images in Y\n","  if mask_path is not None:\n","    masks = sorted(glob(mask_path + os.sep + '*.tif'))\n","    Y = np.zeros((len(masks), IMG_HEIGHT, IMG_WIDTH))\n","    print('Loading the masks ... ')\n","    for n, element in tqdm(enumerate(masks), total = len(masks)):\n","      mask = imread(element)\n","      Y[n,:,:] = mask\n","      \n","    return X,Y\n","  \n","  else:\n","\n","    return X"]},{"cell_type":"markdown","source":["The training and testing sets are defined below. By convention, the raw data are called X and the the labelled images Y."],"metadata":{"id":"FvvsxnyIPMp6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"B0BR0W0FRFos"},"outputs":[],"source":["train_data_path = TRAIN_PATH + os.sep + 'raw' \n","train_mask_path = TRAIN_PATH + os.sep + 'mask'\n","\n","test_data_path = TEST_PATH + os.sep + 'raw'\n","test_mask_path = TEST_PATH + os.sep + 'mask'\n","\n","# load the training and testing set\n","x_train, Y_train = get_data(train_data_path, train_mask_path)\n","x_test, Y_test = get_data(test_data_path, test_mask_path)"]},{"cell_type":"markdown","source":["Since the images were selected from different experiments, the imaging conditions were not always indentical. In order to homogeneize the data and help the network generalize, the raw data are normalized."],"metadata":{"id":"vSjAHkfcPeiv"}},{"cell_type":"code","source":["# normalize the images\n","X_train = np.zeros(x_train.shape)\n","X_test = np.zeros(x_test.shape)\n","\n","for n in tqdm(range(len(X_train))):\n","  x = x_train[n, :, :]\n","  X_train[n, :, :] = (x - np.mean(x)) / np.std(x)\n","\n","for n in tqdm(range(len(X_test))):\n","  x = x_test[n, :, :]\n","  X_test[n, :, :] = (x - np.mean(x)) / np.std(x)\n","\n","# X_train = normalize(X_train)\n","# X_test = normalize(X_test)"],"metadata":{"id":"DA4TX4VHPaaG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## II- Data vizualization :\n","\n","Display an example of images"],"metadata":{"id":"6MYI_bjRQL-f"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7K3vFjPDQ0Ax"},"outputs":[],"source":["ix = random.randint(0, len(X_train)-1)\n","print(X_train.shape)\n","im = X_train[ix,:,:]\n","plt.figure(figsize=(10,10))\n","plt.subplot(1,2,1)\n","plt.imshow(im)\n","plt.subplot(1,2,2)\n","plt.imshow(Y_train[ix,:,:])\n","\n","print(f'The max pixel value for the normalized image is {np.max(im)} and min is {np.min(im)}')"]},{"cell_type":"markdown","source":["As illustrated above, there is a strong class imbalance between background (0) and cells (1). This is to keep in mind during the training in order to interpret the results. Indeed, even if the network is systematically assigning the background label to all pixels, the average accuracy of the network will be >87%!"],"metadata":{"id":"9kzLEHeqQRjy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LPucBnaRZu00"},"outputs":[],"source":["# Analyze the class imbalance\n","unique, counts = np.unique(Y_test, return_counts=True)\n","print(f'The background represents {counts[0]/np.sum(counts)*100}% of the total number of pixels')"]},{"cell_type":"markdown","metadata":{"id":"iUIFGC2eSSMN"},"source":["## III - Building a U-Net from scratch :\n","\n","As for the previous convolutional network used for image classification, the Unet is still built by sequentially adding convolutional blocks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"buNoOAHPSRwl"},"outputs":[],"source":["# Each block of u-net architecture consists of two Convolution layers\n","# These two layers are written in a function to make our code clean\n","# -----------------------------------------------------------------\n","\n","def conv2d_block(input_tensor, n_filters, kernel_size=3):\n","\n","    # first layer\n","    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size),padding=\"same\")(input_tensor)\n","    x = Activation(\"relu\")(x)\n","    x = BatchNormalization()(x)\n","\n","    # second layer\n","    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), padding=\"same\")(x)\n","    x = Activation(\"relu\")(x)\n","    x = BatchNormalization()(x)\n","    \n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AY-T02qJSZ15"},"outputs":[],"source":["# The u-net architecture consists of contracting and expansive paths which\n","# shrink and expands the in/out image respectivly. The output image have the \n","# same size of the input image.\n","# -----------------------------\n","\n","def get_unet(input_img, n_filters):\n","\n","    # contracting path\n","    # ----------------\n","    c1 = conv2d_block(input_img, n_filters=n_filters*4, kernel_size=3) #The first block of U-net\n","    p1 = MaxPooling2D((2, 2)) (c1)\n","\n","    c2 = conv2d_block(p1, n_filters=n_filters*8, kernel_size=3)\n","    p2 = MaxPooling2D((2, 2)) (c2)\n","\n","    c3 = conv2d_block(p2, n_filters=n_filters*16, kernel_size=3)\n","    p3 = MaxPooling2D((2, 2)) (c3)\n","\n","    c4 = conv2d_block(p3, n_filters=n_filters*32, kernel_size=3)\n","    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n","\n","    c5 = conv2d_block(p4, n_filters=n_filters*64, kernel_size=3)\n","    #p5 = MaxPooling2D(pool_size=(2, 2)) (c5)\n","    \n","    # expansive path\n","    # --------------\n","    u7 = Conv2DTranspose(n_filters*64, (3, 3), strides=(2, 2), padding='same') (c5) #upsampling included\n","    u7 = Concatenate()([u7, c4])\n","    c7 = conv2d_block(u7, n_filters=n_filters*64, kernel_size=3)\n","    #c7 = Dropout(0.4) (c7)\n","\n","    u8 = Conv2DTranspose(n_filters*32, (3, 3), strides=(2, 2), padding='same') (c7)\n","    u8 = Concatenate()([u8, c3])\n","    c8 = conv2d_block(u8, n_filters=n_filters*32, kernel_size=3)\n","    #c8 = Dropout(0.35) (c8)\n","\n","    u9 = Conv2DTranspose(n_filters*16, (3, 3), strides=(2, 2), padding='same') (c8)\n","    u9 = Concatenate()([u9, c2])\n","    c9 = conv2d_block(u9, n_filters=n_filters*16, kernel_size=3)\n","    #c9 = Dropout(0.3) (c9)\n","\n","    u10 = Conv2DTranspose(n_filters*8, (3, 3), strides=(2, 2), padding='same') (c9)\n","    u10 = Concatenate()([u10, c1])\n","    c10 = conv2d_block(u10, n_filters=n_filters*8, kernel_size=3)\n","    #c10 = Dropout(0.25) (c10)\n","    \n","    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c10)\n","    model = Model(inputs=[input_img], outputs=[outputs])\n","    return model"]},{"cell_type":"markdown","source":["Create the Unet:"],"metadata":{"id":"HIT5h82cHB3W"}},{"cell_type":"code","source":["input_img = Input((X_train.shape[1], X_train.shape[2],1), name='img')\n","model = get_unet(input_img, n_filters=4)\n","\n","model.summary()"],"metadata":{"id":"Jp5U2H21HA1z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Implementation of the dice coefficient metrics for the training and the loss."],"metadata":{"id":"hE80f3h-aIjI"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Limwnm7SfKx"},"outputs":[],"source":["# the coefficient takes values in [0, 1], where 0 is the worst score, 1 is the \n","# best score the dice coefficient of two sets represented as vectors a, b is \n","# computed as (2 *|a b| / (a + b))\n","# ------------------------------------\n","\n","def dice_coefficient(y_true, y_pred):\n","    eps = 1e-6\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    out = (2. * intersection) / (K.sum(y_true_f) + \\\n","                                 K.sum(y_pred_f) + eps)\n","    return out\n","\n","def loss_dice_coefficient(y_true, y_pred):\n","    eps = 1e-6\n","    y_true_f = K.flatten(y_true)\n","    y_pred_f = K.flatten(y_pred)\n","    y_true_f = float(y_true_f)\n","    y_pred_f = float(y_pred_f)\n","    intersection = K.sum(y_true_f * y_pred_f)\n","    out = 1 - (2. * intersection) / (K.sum(y_true_f) + \\\n","                                     K.sum(y_pred_f) + eps)\n","    return out"]},{"cell_type":"markdown","source":["Compile the network with the selected optimizer and loss function and launch the training."],"metadata":{"id":"RaK-zoosaddx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qm6H7My7SiiS"},"outputs":[],"source":["model.compile(optimizer='adam', \n","              loss=loss_dice_coefficient,\n","              metrics=[dice_coefficient])\n","\n","# model.compile(optimizer='adam', \n","#               loss=\"binary_crossentropy\",\n","#               metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iU_dKlxDbnXP"},"outputs":[],"source":["results = model.fit(X_train, Y_train, \n","                    epochs=250, \n","                    validation_split=0.2,\n","                    batch_size=32,\n","                    shuffle=True,\n","                    )"]},{"cell_type":"markdown","source":["## IV - Evaluate the performance of the network :\n","\n","Plot the loss and metrics curves. "],"metadata":{"id":"U0fx4OIvpxSR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dwS0nXpbbsXI"},"outputs":[],"source":["history_dict = results.history\n","\n","# acc_values = history_dict['accuracy']\n","# val_acc_values = history_dict['val_accuracy']\n","acc_values = history_dict['dice_coefficient']\n","val_acc_values = history_dict['val_dice_coefficient']\n","\n","n = len(acc_values)\n","epochs = range(1, n+1)\n","\n","plt.plot(epochs, acc_values, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Dice coefficient')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jge2wvRXbtIQ"},"outputs":[],"source":["loss_values = history_dict['loss']\n","val_loss_values = history_dict['val_loss']\n","\n","plt.plot(epochs, loss_values, '-bo', label='Training loss')\n","plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","source":["Calculate the average dice coefficient calculated over the testing set."],"metadata":{"id":"rFZLe7c4bRfH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"z1hWa1F-bw_2"},"outputs":[],"source":["model.evaluate(X_test,Y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2WvRtHNHbzmy"},"outputs":[],"source":["preds_test = model.predict(X_test, verbose=1)\n","# we apply a threshold on predicted mask (probability mask) to convert it to a binary mask.\n","preds_test_t = (preds_test > 0.5).astype(np.uint8) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gM31K3s6b2JJ"},"outputs":[],"source":["ix = random.randint(0, len(X_test)-1)\n","\n","fig = plt.figure(figsize=(10, 10))\n","plt.subplot(221)\n","plt.imshow(X_test[ix,:,:])\n","plt.title(\"input image\")\n","\n","plt.subplot(222)\n","plt.imshow(np.squeeze(Y_test[ix, :, :]))\n","plt.title(\"ground truth\")\n","\n","plt.subplot(223)\n","plt.imshow(np.squeeze(preds_test[ix,:,:,0]))\n","plt.title(\"Probability map of the predicted mask\")\n","\n","plt.subplot(224)\n","plt.imshow(np.squeeze(preds_test_t[ix, :, :]))\n","plt.title(\"Predicted mask after thresholding\")\n","\n","plt.show()"]}],"metadata":{"colab":{"provenance":[{"file_id":"1ULhAHEmjGMwpNLSj85g8v6xkWhEvqEJj","timestamp":1668697759380},{"file_id":"1ZkTIoTxpyCOFhvPXItw1PQguwfbUmOqk","timestamp":1654101515000},{"file_id":"1FgTpjXoHekWYQO1ojL86fbWbXqgAz0v6","timestamp":1652796725832}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}